{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all related to predictive modeling and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methodology used is an adaptation of Crisp-DM. Currently, we are the steps Modeling and Evaluation:\n",
    "1. Domain Understanding\n",
    "2. Data Understanding\n",
    "4. Data Preparation\n",
    "5. **Modeling**\n",
    "6. **Evaluation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\benjamin\\.conda\\envs\\scikit-jupyter\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning:\n",
      "\n",
      "sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_validate\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_decomposition\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import scale\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "from skrebate import SURF\n",
    "import pickle\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_url =  \"https://raw.githubusercontent.com/Naio/aasa-stability-prediction/master/data/processed/\"\n",
    "seed = 10 #Seed for controlling any random procedure during the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator that yields ready-to-use datasets\n",
    "def datasets():\n",
    "    datasets_names = ['p1STN', 'p4LYZ', 'p1BPI', 'HLYZ']\n",
    "    data_url = 'https://raw.githubusercontent.com/Naio/aasa-stability-prediction/master/data/processed/original/'\n",
    "    \n",
    "    for dataset_name in datasets_names:\n",
    "        protein_dataset = pd.read_csv(data_url + dataset_name + '.csv')\n",
    "        #protein_dataset = preprocess_dataset(protein_dataset)\n",
    "        protein_dataset = normalize_data(protein_dataset)\n",
    "        features, target = split_features_and_target(protein_dataset)\n",
    "        \n",
    "        yield {'name': dataset_name, 'features': features, 'target': target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(df):\n",
    "    \n",
    "    #Discard of those descriptors that are highly correlated\n",
    "    df = discard_highly_correlated_descriptors(df)\n",
    "    \n",
    "    #Feature selection\n",
    "    #Note: This implementation is able to work even if data is not normalized.\n",
    "    df = select_subset_of_features_SURF(df, number_of_features)\n",
    "    \n",
    "    #Z-Score Normalization\n",
    "    df = normalize_data(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discards descriptors that are highly correlated at least with one other descriptor.\n",
    "#The threshold is an absolute Pearson's r greater than 0.99 \n",
    "def discard_highly_correlated_descriptors(df):\n",
    "    \n",
    "    #Calculates the absolute Pearson's r correlation matrix. Both -1 and 1 are highly correlated.\n",
    "    correlations = df.corr().abs()\n",
    "    \n",
    "    #Gets the correlation matrix upper triangular.\n",
    "    upper_corr = correlations.where(np.triu(np.ones(correlations.shape), k=1).astype(np.bool))\n",
    "    \n",
    "    #Discards the descriptors\n",
    "    to_drop = [column for column in upper_corr.columns if any(upper_corr[column] > 0.99)]\n",
    "    return df.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subset_of_features_SURF(df, n_features):\n",
    "    features_importance = calculate_features_importance(df, n_features)\n",
    "    selected_features = select_n_most_important_features(features_importance, n_features)\n",
    "    filtered_df = filter_selected_columns(df, selected_features)\n",
    "    return filtered_df\n",
    "\n",
    "def calculate_features_importance(df, n_features):\n",
    "    features, target = split_features_and_target(df)\n",
    "    \n",
    "    rlf = SURF(n_features_to_select=n_features)\n",
    "    rlf.fit(features, target)\n",
    "    \n",
    "    return pd.DataFrame({'feature_name':df.iloc[:, 2:].columns, \n",
    "                         'importance': rlf.feature_importances_})\n",
    "\n",
    "def select_n_most_important_features(features_importance, n_features):\n",
    "    return features_importance.sort_values(by='importance', ascending=False).head(n_features)['feature_name'].tolist()\n",
    "\n",
    "def filter_selected_columns(df, columns):\n",
    "    selected = ['id', 'stability']\n",
    "    selected.extend(columns)\n",
    "    return df[selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a dataset with normalized descriptors using Z-Score\n",
    "def normalize_data(df):\n",
    "    \n",
    "    #Only descriptores are normalized, so we set apart the stability and mutation name attributes from the original dataset\n",
    "    mutation_stability = df.iloc[:, 0:2]\n",
    "    \n",
    "    #Setting apart the descriptors data\n",
    "    descriptors = df.iloc[:, 2:]\n",
    "                      \n",
    "    #Normalizing the descriptors using Z-Score\n",
    "    normalized_descriptors = pd.DataFrame(scale(descriptors), columns=descriptors.columns)\n",
    "    \n",
    "    #Joining stability and mutation name to the normalized descriptors\n",
    "    normalized_data = mutation_stability.join(normalized_descriptors)\n",
    "    \n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns descriptors and target values as numpy arrays\n",
    "def split_features_and_target(df):\n",
    "    features = df.iloc[:, 2:].to_numpy()\n",
    "    target =  df.iloc[:,1].to_numpy()\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining hyperparameter grids\n",
    "Each algorithm has its corresponding hyperparameter grid for later use in grid search inner cross-validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_squares_grid = {} #Ordinary least square doesn't have hyperparamters\n",
    "\n",
    "alpha_range = np.logspace(-6, 6, 13)\n",
    "ridge_grid = {'alpha' : alpha_range} #Alpha between 1.e-06 and 1.e+06\n",
    "lasso_grid = {'alpha' : alpha_range} \n",
    "\n",
    "pls_grid = {'n_components': np.linspace(start = 2, stop=25, num=100).astype(int)} #Between 2 and 25 Principal Components\n",
    "c_range = np.logspace(-6, 6, 13) #Between 1.e-05 and 1.e+02. Lower C, more regularization. np.logspace(-3, 3, 7)\n",
    "gamma_range = np.logspace(-6, 6, 13)\n",
    "epsilon_range = np.linspace(start = 1.0, stop=2.5, num=16)#np.linspace(start = 1.0, stop=2.5, num=16)\n",
    "svr_grid = [\n",
    "    #Grid for rbf and sigmoid kernel\n",
    "    {'C': c_range, 'gamma': gamma_range, 'kernel': ['rbf'], 'epsilon': epsilon_range},\n",
    "    #Grid for polinomial kernel\n",
    "    {'C': c_range, 'gamma': gamma_range, 'kernel': ['poly'], 'degree': [2,3], 'epsilon': epsilon_range}\n",
    " ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating estimators for each learning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_methods function will create the empty estimators and map them to their corresponding hyperparameter grid.\n",
    "def get_learning_methods():\n",
    "    learning_methods = [{'name': 'OLS', 'estimator': linear_model.LinearRegression(), 'hyperparameter_grid': least_squares_grid},\n",
    "                        {'name': 'RIDGE','estimator':linear_model.Ridge(random_state=seed), 'hyperparameter_grid': ridge_grid},\n",
    "                        {'name': 'LASSO', 'estimator': linear_model.Lasso(max_iter=100000), 'hyperparameter_grid': lasso_grid},\n",
    "                        {'name': 'PLS', 'estimator': cross_decomposition.PLSRegression(scale=False), 'hyperparameter_grid': pls_grid},\n",
    "                        {'name': 'SVR', 'estimator': svm.SVR(), 'hyperparameter_grid': svr_grid}]#tol=0.01, max_iter=500000\n",
    "    return learning_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv(features, target):\n",
    "    \"\"\"\n",
    "    Performs nested cross-validation over the given dataset, for each learning method defined.\n",
    "    \n",
    "    Reports the scores, over both train and test sets, calculated in the outer cross-validation loop.\n",
    "    \n",
    "    Parameters:\n",
    "    features: A numpy array of shape (n_samples, n_features) containing dataset features.\n",
    "    target: A numpy array of shape (n_samples, n_features) containing dataset target variable.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary with keys 'test_scores', 'training_scores', containing the scores calculated in the outer cv loop.\n",
    "    \"\"\"\n",
    "    #     \n",
    "    #Score metric used for hyperparameter optimization in inner CV loop\n",
    "    inner_scoring = 'neg_mean_squared_error'\n",
    "    \n",
    "    learning_methods = get_learning_methods()\n",
    "    \n",
    "    results = {}\n",
    "    for learning_method in learning_methods:\n",
    "        \n",
    "        print(\"----\"+ learning_method['name'] + \"------\")\n",
    "        \n",
    "        #Setting a seed ensures that each learning method will be trained on the same splits.\n",
    "        inner_cv = KFold(n_splits=10, shuffle=True, random_state=seed + 1)\n",
    "        outer_cv = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        \n",
    "        #Contains data about the results for a particular learning method.\n",
    "        learning_method_results = {}\n",
    "        \n",
    "        learning_method_results['best_parameters'] = []\n",
    "        learning_method_results['train_scores'] = {'R-Squared': [], 'RMSE': []}\n",
    "        learning_method_results['test_scores'] = {'R-Squared': [], 'RMSE': []}\n",
    "        \n",
    "        #split() method returns a generator that gives all cross-validation partitions. \n",
    "        for train_index, test_index in outer_cv.split(features):\n",
    "            \n",
    "            #Split the data between train and test sets\n",
    "            train_features, test_features = features[train_index], features[test_index]\n",
    "            train_target, test_target = target[train_index], target[test_index]\n",
    "            \n",
    "            \n",
    "            #When the fit() method is called, it will internally perform a grid search cross-validation. \n",
    "            #Once it finds the best hyperparameters, it will fit on complete training set using those parameters.\n",
    "            grid_search_estimator = GridSearchCV(estimator = learning_method['estimator'], \n",
    "                           param_grid = learning_method['hyperparameter_grid'], \n",
    "                           cv = inner_cv, \n",
    "                           scoring = inner_scoring,\n",
    "                           #When n_jobs is -1, all CPUs are used to run cross-validation in parallel\n",
    "                           n_jobs=-1)\n",
    "            \n",
    "            grid_search_estimator.fit(train_features, train_target)\n",
    "            best_parameters = grid_search_estimator.best_params_\n",
    "            learning_method_results['best_parameters'].append(best_parameters)\n",
    "            \n",
    "            #Prediction using the best estimator selected via Grid Search CV\n",
    "            train_prediction = grid_search_estimator.predict(train_features)\n",
    "            test_prediction = grid_search_estimator.predict(test_features)\n",
    "            \n",
    "            \n",
    "            #Calculating R-Squared score\n",
    "            train_r2 = r2_score(y_true = train_target, y_pred = train_prediction)\n",
    "            test_r2 = r2_score(y_true = test_target, y_pred = test_prediction)\n",
    "            \n",
    "            learning_method_results['train_scores']['R-Squared'].append(train_r2)\n",
    "            learning_method_results['test_scores']['R-Squared'].append(test_r2)\n",
    "            \n",
    "            #Calculating RMSE score\n",
    "            train_rmse =  mean_squared_error(y_true = train_target, y_pred = train_prediction, squared=False)\n",
    "            test_rmse = mean_squared_error(y_true = test_target, y_pred = test_prediction, squared=False)\n",
    "            \n",
    "            learning_method_results['train_scores']['RMSE'].append(train_rmse)\n",
    "            learning_method_results['test_scores']['RMSE'].append(test_rmse)\n",
    "            \n",
    "        \n",
    "            \n",
    "        #Stores results for a particular learning method\n",
    "        results[learning_method['name']] = learning_method_results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receives the results for a specific dataset and transform them into more readeable dataframes\n",
    "def extract_nestedcv_results(results):\n",
    "    \n",
    "    learning_methods_names = ['OLS', 'RIDGE', 'LASSO', 'PLS', 'SVR']\n",
    "    \n",
    "    #Groups test scores of every learning method in one dictionary per metric.\n",
    "    train_r2 = {method_name:results[method_name]['train_scores']['R-Squared'] for method_name in learning_methods_names} \n",
    "    train_rmse = {method_name:results[method_name]['train_scores']['RMSE'] for method_name in learning_methods_names}\n",
    "    \n",
    "    #Groups test scores of every learning method in one dictionary per metric.\n",
    "    test_r2 = {method_name:results[method_name]['test_scores']['R-Squared'] for method_name in learning_methods_names} \n",
    "    test_rmse = {method_name:results[method_name]['test_scores']['RMSE'] for method_name in learning_methods_names}\n",
    "    \n",
    "    \n",
    "    #For each learning algorithm, groups best parameters selected in each iteration of Nested CV outer loop\n",
    "    best_parameters = {method_name:results[method_name]['best_parameters'] for method_name in learning_methods_names}\n",
    "    \n",
    "    return {'train_r2': pd.DataFrame(train_r2), 'train_rmse': pd.DataFrame(train_rmse),\n",
    "           'test_r2': pd.DataFrame(test_r2), 'test_rmse': pd.DataFrame(test_rmse), \n",
    "           'best_parameters': pd.DataFrame(best_parameters)}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More naive approaches to hyperparameter optimization and algorithm comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The nested_cv function returns a dictionary like with the outer cross-validation loop scores for every learning method.\n",
    "#The dictionary looks like:\n",
    "#{\n",
    "#    'r2': {'PLS': [0.99,...,0.67], 'SVR': [0.94,..., 0.98], ... , 'OLS': [0.4, ..., 0.32]}, \n",
    "#  'rmse': {'PLS': [1.297116,...,2.297116], 'SVR': [1.291,..., 0.29471], ... , 'OLS': [3.19283, ..., 5.827391]}\n",
    "#}\n",
    "def naive_approach(features, target, k_value = 10):\n",
    "    \n",
    "    #Score metric used for hyperparameter optimization in inner CV loop\n",
    "    inner_scoring = 'neg_root_mean_squared_error'\n",
    "    \n",
    "    #Score metrics used in outer CV loop for generalization performance estimation of the learning method \n",
    "    outer_scoring = ['r2', 'neg_root_mean_squared_error']\n",
    "    \n",
    "    test_r2 = {}\n",
    "    train_r2 = {}\n",
    "    test_rmse = {}\n",
    "    train_rmse = {}\n",
    "    best_hyperparameters = {}\n",
    "    \n",
    "    learning_methods = get_learning_methods()\n",
    "    \n",
    "    for learning_method in learning_methods:\n",
    "        #gridsearch_cv = KFold(n_splits=k_value, shuffle=True, random_state=seed + 1)\n",
    "        #comparison_cv = KFold(n_splits=k_value, shuffle=True, random_state=seed)\n",
    "        \n",
    "        gridsearch_cv = FiveForTwoKFold(features, seed + 1)\n",
    "        comparison_cv = FiveForTwoKFold(features, seed)\n",
    "        \n",
    "        grid_search = GridSearchCV(estimator = learning_method['estimator'], \n",
    "                           param_grid = learning_method['hyperparameter_grid'], \n",
    "                           cv = gridsearch_cv, \n",
    "                           scoring = inner_scoring,\n",
    "                           #When n_jobs is -1, all CPUs are used to run cross-validation in parallel\n",
    "                           n_jobs=-1)\n",
    "        \n",
    "        #When the fit() method is called, it will internally perform a grid search cross-validation.\n",
    "        grid_search.fit(features, target)\n",
    "\n",
    "        #The best model/hyperparameters are evaluated on a cross-validation process\n",
    "        cv_results = cross_validate(estimator = grid_search.best_estimator_, \n",
    "                                          X = features, y = target, \n",
    "                                          cv = comparison_cv, scoring = outer_scoring,\n",
    "                                          return_train_score=True)\n",
    "        \n",
    "        test_r2[learning_method['name']] = cv_results['test_r2'].tolist()\n",
    "        train_r2[learning_method['name']] = cv_results['train_r2'].tolist()\n",
    "        \n",
    "        #Inside CV, the RSME score is managed as a negative RMSE. Multiplying it by -1 will turn it into the usual positive RMSE  \n",
    "        test_rmse[learning_method['name']] = (cv_results['test_neg_root_mean_squared_error']*-1).tolist() \n",
    "        train_rmse[learning_method['name']] = (cv_results['train_neg_root_mean_squared_error']*-1).tolist()\n",
    "        \n",
    "        \n",
    "        best_hyperparameters[learning_method['name']] = grid_search.best_params_\n",
    "        \n",
    "        \n",
    "    return {'train_r2': train_r2, 'train_rmse': train_rmse, \n",
    "            'test_r2': test_r2, 'test_rmse':test_rmse, \n",
    "            'best_hyperparameters': best_hyperparameters}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5x2 CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FiveForTwoKFold(data, seed):\n",
    "    for i in range(5):\n",
    "        kfold = KFold(n_splits=2, shuffle=True, random_state=seed + i)\n",
    "        for train_index, test_index in kfold.split(data):\n",
    "            yield train_index, test_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments():\n",
    "    nested_cv_results = {}\n",
    "    k10_results = {}\n",
    "    k5_results = {}\n",
    "    \n",
    "    for dataset in datasets():\n",
    "        print(\"Running experiments in dataset \", dataset['name'])\n",
    "        k5_results[dataset['name']] = naive_approach(dataset['features'], dataset['target'], k_value=5)\n",
    "        nested_cv_results[dataset['name']] =  k5_results[dataset['name']]  #nested_cv(dataset['features'], dataset['target'])\n",
    "        k10_results[dataset['name']] = k5_results[dataset['name']] #naive_approach(dataset['features'], dataset['target'], k_value=10)\n",
    "    \n",
    "    return {'nested_cv': nested_cv_results, '10cv': k10_results, '5cv': k5_results}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments with feature selection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_subset_sizes = [60, 50, 40, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Number of Features ; 60---------------------\n",
      "Running experiments in dataset  p1STN\n",
      "Running experiments in dataset  p4LYZ\n",
      "Running experiments in dataset  p1BPI\n",
      "Running experiments in dataset  HLYZ\n",
      "----------------- Number of Features ; 50---------------------\n",
      "Running experiments in dataset  p1STN\n",
      "Running experiments in dataset  p4LYZ\n",
      "Running experiments in dataset  p1BPI\n",
      "Running experiments in dataset  HLYZ\n",
      "----------------- Number of Features ; 40---------------------\n",
      "Running experiments in dataset  p1STN\n",
      "Running experiments in dataset  p4LYZ\n",
      "Running experiments in dataset  p1BPI\n",
      "Running experiments in dataset  HLYZ\n",
      "----------------- Number of Features ; 30---------------------\n",
      "Running experiments in dataset  p1STN\n",
      "Running experiments in dataset  p4LYZ\n",
      "Running experiments in dataset  p1BPI\n",
      "Running experiments in dataset  HLYZ\n"
     ]
    }
   ],
   "source": [
    "for number_of_features in feature_subset_sizes:\n",
    "    print(\"----------------- Number of Features ; \" + str(number_of_features) + \"---------------------\")\n",
    "    results = run_experiments()\n",
    "    \n",
    "#     dump_results(results['nested_cv'], results_dir +str(number_of_features) +'fs_nestedCV.pickle')\n",
    "#     dump_results(results['10cv'], results_dir + str(number_of_features) +'fs_10CV.pickle')\n",
    "    dump_results(results['5cv'], results_dir + str(number_of_features) +'fs_5CV.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a results directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = '../../results/modeling/fivefortwo/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serializing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumps a dictionary with the results into a text file so it can be loaded later in Python.\n",
    "def dump_results(results, filename):\n",
    "    with open(filename, 'wb') as results_file:\n",
    "        pickle.dump(results, results_file)\n",
    "\n",
    "    loaded_results = load_dumped_results(filename)\n",
    "\n",
    "    #If the original results and the dumped ones are not the same\n",
    "    if(results != loaded_results):\n",
    "        raise Exception('There was a error. The loaded dumped results are not the same as the original.')\n",
    "        \n",
    "#Loads the dumped results\n",
    "def load_dumped_results(filename):\n",
    "    with open(filename, 'rb') as results_file:\n",
    "        loaded_results = pickle.load(results_file)\n",
    "    return loaded_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities for exporting the results into latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'mean'\n",
    "fivefortwo = load_dumped_results('../../results/modeling/fivefortwo/transformed/pca/_5x2CV.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "fivefortwo = load_dumped_results('../../results/modeling/fivefortwo/40fs_5CV.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'mean'\n",
    "nested = load_dumped_results('../../results/modeling/first_experiments/'+ experiment +'_nestedCV.pickle')\n",
    "tencv = load_dumped_results('../../results/modeling/first_experiments/'+ experiment +'_10CV.pickle')\n",
    "fivecv = load_dumped_results('../../results/modeling/first_experiments/'+ experiment +'_5CV.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = str(40)\n",
    "nested = load_dumped_results('../../results/modeling/pearson99/'+ experiment +'fs_nestedCV.pickle')\n",
    "tencv = load_dumped_results('../../results/modeling/pearson99/'+ experiment +'fs_10CV.pickle')\n",
    "fivecv = load_dumped_results('../../results/modeling/pearson99/'+ experiment +'fs_5CV.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested = load_dumped_results('../../results/modeling/fivefortwo/40fs_5CV.pickle')\n",
    "tencv = load_dumped_results('../../results/modeling/fivefortwo/40fs_5CV.pickle')\n",
    "fivecv = load_dumped_results('../../results/modeling/fivefortwo/40fs_5CV.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'----- 1STN Dataset------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'OLS': {},\n",
       " 'RIDGE': {'alpha': 100.0},\n",
       " 'LASSO': {'alpha': 0.1},\n",
       " 'PLS': {'n_components': 2},\n",
       " 'SVR': {'C': 10.0, 'epsilon': 1.3, 'gamma': 0.01, 'kernel': 'rbf'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'----- 4LYZ Dataset------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'OLS': {},\n",
       " 'RIDGE': {'alpha': 100.0},\n",
       " 'LASSO': {'alpha': 1.0},\n",
       " 'PLS': {'n_components': 2},\n",
       " 'SVR': {'C': 10.0, 'epsilon': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'----- 1BPI Dataset------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'OLS': {},\n",
       " 'RIDGE': {'alpha': 100.0},\n",
       " 'LASSO': {'alpha': 0.1},\n",
       " 'PLS': {'n_components': 2},\n",
       " 'SVR': {'C': 100.0, 'epsilon': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display('----- 1STN Dataset------')\n",
    "display(fivecv['p1STN']['best_hyperparameters'])\n",
    "display('----- 4LYZ Dataset------')\n",
    "display(fivecv['p4LYZ']['best_hyperparameters'])\n",
    "display('----- 1BPI Dataset------')\n",
    "display(fivecv['p1BPI']['best_hyperparameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1STN {'C': 10.0, 'epsilon': 1.3, 'gamma': 0.01, 'kernel': 'rbf'}\n",
    "#4LYZ {'C': 10.0, 'epsilon': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}}\n",
    "#1BPI {'C': 100.0, 'epsilon': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Nested Results-----------------------\n",
      "\\begin{tabular}{lrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} &  score1mean &  score1median &  score1stdev &  score1max &  score1min &  score2mean &  score2median &  score2stdev &  score2max &  score2min \\\\\n",
      "\\midrule\n",
      "OLS   &        0.12 &          0.20 &         0.33 &       0.50 &      -0.51 &        3.98 &          3.53 &         0.92 &       5.53 &       3.06 \\\\\n",
      "RIDGE &        0.23 &          0.29 &         0.27 &       0.51 &      -0.17 &        3.73 &          3.46 &         0.81 &       4.85 &       2.44 \\\\\n",
      "LASSO &        0.18 &          0.32 &         0.44 &       0.64 &      -0.56 &        3.75 &          3.82 &         0.92 &       5.43 &       2.42 \\\\\n",
      "PLS   &        0.19 &          0.22 &         0.27 &       0.56 &      -0.30 &        3.85 &          3.68 &         1.01 &       5.97 &       2.60 \\\\\n",
      "SVR   &        0.42 &          0.47 &         0.27 &       0.76 &      -0.06 &        3.13 &          3.16 &         0.43 &       3.98 &       2.56 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "---------------------- 10Fold Results-----------------------\n",
      "\\begin{tabular}{lrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} &  score1mean &  score1median &  score1stdev &  score1max &  score1min &  score2mean &  score2median &  score2stdev &  score2max &  score2min \\\\\n",
      "\\midrule\n",
      "OLS   &        0.12 &          0.20 &         0.33 &       0.50 &      -0.51 &        3.98 &          3.53 &         0.92 &       5.53 &       3.06 \\\\\n",
      "RIDGE &        0.26 &          0.29 &         0.30 &       0.66 &      -0.17 &        3.61 &          3.46 &         0.75 &       4.68 &       2.44 \\\\\n",
      "LASSO &        0.25 &          0.30 &         0.29 &       0.64 &      -0.15 &        3.63 &          3.45 &         0.74 &       4.75 &       2.62 \\\\\n",
      "PLS   &        0.24 &          0.27 &         0.33 &       0.70 &      -0.39 &        3.61 &          3.74 &         0.67 &       4.71 &       2.64 \\\\\n",
      "SVR   &        0.47 &          0.48 &         0.25 &       0.78 &      -0.06 &        2.98 &          3.02 &         0.37 &       3.59 &       2.43 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "---------------------- 5Fold Results-----------------------\n",
      "\\begin{tabular}{lrrrrrrrrrr}\n",
      "\\toprule\n",
      "{} &  score1mean &  score1median &  score1stdev &  score1max &  score1min &  score2mean &  score2median &  score2stdev &  score2max &  score2min \\\\\n",
      "\\midrule\n",
      "OLS   &        0.11 &         -0.03 &         0.23 &       0.40 &      -0.08 &        4.24 &          4.11 &         0.55 &       4.90 &       3.49 \\\\\n",
      "RIDGE &        0.28 &          0.24 &         0.12 &       0.42 &       0.13 &        3.84 &          3.92 &         0.52 &       4.42 &       2.99 \\\\\n",
      "LASSO &        0.28 &          0.20 &         0.16 &       0.46 &       0.13 &        3.83 &          3.87 &         0.45 &       4.41 &       3.18 \\\\\n",
      "PLS   &        0.29 &          0.33 &         0.24 &       0.60 &      -0.05 &        3.73 &          3.75 &         0.32 &       4.07 &       3.30 \\\\\n",
      "SVR   &        0.48 &          0.49 &         0.13 &       0.67 &       0.35 &        3.25 &          3.13 &         0.50 &       3.80 &       2.72 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_results(nested, tencv, fivecv, 'HLYZ', test=True, to_latex=True, info='scores_summary') #scores_summary, iteration_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If test=false Returns train scores instead of test scores. If to_latex is False, then it returns the dataframes with\n",
    "#the results. \n",
    "#info can take the values 'iteration_scores','scores_summary', 'both'.\n",
    "def show_results(nested_results, tenfold_results, fivefold_results, dataset_name, test=True, to_latex=True, info='scores_summary'):\n",
    "    \"\"\"\n",
    "    params: \n",
    "        nested_results: Results returned by the nested_cv function.\n",
    "        tenfold_results: Results returned by naive_approach function with k=10\n",
    "        fivefold_results Results returned by naive_approach function with k=5\n",
    "        dataset_name: A string with the name of the dataset which results are needed. Possible values are 'p1STN', 'p4LYZ', 'p1BPI', 'HLYZ'.\n",
    "        test: A boolean value. If True, then it returns the test scores. If False, then it return train scores.\n",
    "        to_latex: A boolean value. If True, it prints the results as latex-formatted tables. If False, results are displayed as Pandas DataFrames.\n",
    "        info: A string that indicates what kind of information it is needed. 'scores_summary', \n",
    "        mean, median, stdev, max and min statistics are showed. If it is 'iteration_scores', then individual scores for each cross-validation iteration are showed.\n",
    "        If 'both', then function shows both individual scores and summary statistics.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if(test):\n",
    "        score_prefix = 'test_'\n",
    "    else:\n",
    "        score_prefix = 'train_'\n",
    "    \n",
    "    nested_results = nested_results[dataset_name]\n",
    "    tenfold_results = tenfold_results[dataset_name]\n",
    "    fivefold_results = fivefold_results[dataset_name]\n",
    "    \n",
    "    \n",
    "    #Getting test scores \n",
    "    nested_r2, tenfold_r2, fivefold_r2 = algo(nested_results, tenfold_results, \n",
    "                                             fivefold_results, score=score_prefix + 'r2', \n",
    "                                             info=info)\n",
    "    \n",
    "    \n",
    "    nested_rmse, tenfold_rsme, fivefold_rmse = algo(nested_results, tenfold_results, \n",
    "                                                    fivefold_results, score=score_prefix + 'rmse',\n",
    "                                                    info=info)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(info=='scores_summary'):\n",
    "    \n",
    "        nested_df = concat_scores_statistics(nested_r2, nested_rmse)\n",
    "        tenfold_df = concat_scores_statistics(tenfold_r2, tenfold_rsme)\n",
    "        fivefold_df = concat_scores_statistics(fivefold_r2, fivefold_rmse)\n",
    "    \n",
    "        if(to_latex):\n",
    "            print('---------------------- Nested Results-----------------------')\n",
    "            export_to_latex(nested_df)\n",
    "            print('---------------------- 10Fold Results-----------------------')\n",
    "            export_to_latex(tenfold_df)\n",
    "            print('---------------------- 5Fold Results-----------------------')\n",
    "            export_to_latex(fivefold_df)\n",
    "        else:\n",
    "            print('---------------------- Nested Results-----------------------')\n",
    "            display(nested_df)\n",
    "            print('---------------------- 10Fold Results-----------------------')\n",
    "            display(tenfold_df)\n",
    "            print('---------------------- 5Fold Results-----------------------')\n",
    "            display(fivefold_df)\n",
    "    else: \n",
    "        \n",
    "        if(to_latex):\n",
    "            print('---------------------- Nested Results-----------------------')\n",
    "            print('-----R2 -----')\n",
    "            export_to_latex(nested_r2)\n",
    "            print('-----RMSE -----')\n",
    "            export_to_latex(nested_rmse)\n",
    "            print('---------------------- 10Fold Results-----------------------')\n",
    "            print('-----R2 -----')\n",
    "            export_to_latex(tenfold_r2)\n",
    "            print('-----RMSE -----')\n",
    "            export_to_latex(tenfold_rsme)\n",
    "            print('---------------------- 5Fold Results-----------------------')\n",
    "            print('-----R2 -----')\n",
    "            export_to_latex(fivefold_r2)\n",
    "            print('-----RMSE -----')\n",
    "            export_to_latex(fivefold_rmse)\n",
    "        else: \n",
    "            print('---------------------- Nested Results-----------------------')\n",
    "            print('-----R2 -----')\n",
    "            display(nested_r2)\n",
    "            print('-----RMSE -----')\n",
    "            display(nested_rmse)\n",
    "            print('---------------------- 10Fold Results-----------------------')\n",
    "            print('-----R2 -----')\n",
    "            display(tenfold_r2)\n",
    "            print('-----RMSE -----')\n",
    "            display(tenfold_rsme)\n",
    "            print('---------------------- 5Fold Results-----------------------')\n",
    "            print('-----R2 -----')\n",
    "            display(fivefold_r2)\n",
    "            print('-----RMSE -----')\n",
    "            display(fivefold_rmse)\n",
    "            \n",
    "            \n",
    "#Returns dataframes with the statistics for each validation procedure\n",
    "def algo(nested_results, tenfold_results, fivefold_results, score, info):\n",
    "    \n",
    "    nested = preprocess_results(extract_nestedcv_results(nested_results)[score])\n",
    "    tenfold = preprocess_results(pd.DataFrame(tenfold_results[score]))\n",
    "    fivefold = preprocess_results(pd.DataFrame(fivefold_results[score]))\n",
    "    \n",
    "    if(info == 'iteration_scores'):\n",
    "        nested = nested.iloc[:, 5:]\n",
    "        tenfold = tenfold.iloc[:, 5:]\n",
    "        fivefold = fivefold.iloc[:, 5:]\n",
    "    if(info == 'scores_summary'):\n",
    "        nested = nested.iloc[:,:5]\n",
    "        tenfold = tenfold.iloc[:,:5]\n",
    "        fivefold = fivefold.iloc[:,:5]\n",
    "        \n",
    "    return nested, tenfold, fivefold\n",
    "\n",
    "#Returns a dataframe with the scores concatenated. Used for merge statistics from R2 and RMSE\n",
    "#into the same dataframe.\n",
    "def concat_scores_statistics(score1, score2):\n",
    "    #A different  prefix is added to the columns names of each dataframe so they have no conflicts.\n",
    "    return pd.concat([add_prefix_to_columns(score1, \"score1\"), add_prefix_to_columns(score2, \"score2\")],axis=1)\n",
    "\n",
    "#Used to add a prefix to the column names. \n",
    "def add_prefix_to_columns(df, prefix):\n",
    "    return df.rename(columns={column: prefix + column for column in df.columns.to_list()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess results before exporting them. Include the calculation of statistics and rounding.\n",
    "def preprocess_results(results):\n",
    "    results = calculate_statistics(results).round(decimals=2)\n",
    "    return results\n",
    "\n",
    "\n",
    "#This function calculates statistics (mean, median, stdev, max and min)\n",
    "#based on the results. Returns a transposed dataframe with the statistics appended to it\n",
    "def calculate_statistics(df):\n",
    "    r = df.T\n",
    "    r = r.rename(columns = {column:'I' + str(column + 1) for column in r.columns.tolist()})\n",
    "    \n",
    "    \n",
    "    r.insert(loc=0, column='min', value= df.T.min(axis='columns'))\n",
    "    r.insert(loc=0, column='max', value= df.T.max(axis='columns'))\n",
    "    r.insert(loc=0, column='stdev', value= df.T.std(axis='columns'))\n",
    "    r.insert(loc=0, column='median', value=df.T.median(axis='columns'))\n",
    "    r.insert(loc=0, column='mean', value= df.T.mean(axis='columns'))\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_latex(results):\n",
    "    print(results.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting results to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_excel(results, experiment_name):\n",
    "    file_name = results_dir + experiment_name + '.xlsx'\n",
    "    with pd.ExcelWriter(file_name) as writer:\n",
    "        \n",
    "        for dataset_name in results.keys():\n",
    "            dataset_results = results[dataset_name]\n",
    "            \n",
    "            for metric_name in dataset_results.keys():\n",
    "                sheet_name = dataset_name + \"_\" + metric_name\n",
    "                process_results(pd.DataFrame(dataset_results[metric_name])).to_excel(writer, sheet_name = sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"fs_48Pearson\"\n",
    "export_to_excel(results, \"10FCV_\" + experiment_name)\n",
    "export_to_excel(k5_results, \"5FCV_\" + experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensembles of SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "HLYZ = pd.read_csv('../../data/processed/original/p1BPI.csv')\n",
    "preprocessed_HLYZ = normalize_data(HLYZ)\n",
    "features, target = split_features_and_target(preprocessed_HLYZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<generator object FiveForTwoKFold at 0x00000268BA8F2AC8>,\n",
       "             error_score=nan,\n",
       "             estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3,\n",
       "                           epsilon=0.1, gamma='scale', kernel='rbf',\n",
       "                           max_iter=-1, shrinking=True, tol=0.001,\n",
       "                           verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid=[{'C': array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n",
       "       1.e+02, 1.e+03...\n",
       "       1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]),\n",
       "                          'degree': [2, 3],\n",
       "                          'epsilon': array([1. , 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2,\n",
       "       2.3, 2.4, 2.5]),\n",
       "                          'gamma': array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n",
       "       1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]),\n",
       "                          'kernel': ['poly']}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_root_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_cv = FiveForTwoKFold(features, seed)\n",
    "est = GridSearchCV(estimator = svm.SVR(), \n",
    "                           param_grid = svr_grid, \n",
    "                           cv = grid_search_cv, \n",
    "                           scoring = 'neg_root_mean_squared_error',\n",
    "                           #When n_jobs is -1, all CPUs are used to run cross-validation in parallel\n",
    "                           n_jobs=-1)\n",
    "\n",
    "est.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{n_estimator: []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingRegressor(base_estimator=SVR(C=1000.0, cache_size=200, coef0=0.0,\n",
       "                                    degree=3, epsilon=1.0, gamma=0.0001,\n",
       "                                    kernel='rbf', max_iter=-1, shrinking=True,\n",
       "                                    tol=0.001, verbose=False),\n",
       "                 bootstrap=True, bootstrap_features=False, max_features=0.2,\n",
       "                 max_samples=53, n_estimators=100, n_jobs=-1, oob_score=True,\n",
       "                 random_state=10, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_reg = BaggingRegressor(est.best_estimator_,\n",
    "                          n_estimators=100, max_samples = len(features), n_jobs=-1,\n",
    "                          bootstrap=True, oob_score=True, max_features = .2, random_state=seed)\n",
    "bag_reg.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6178800070265594"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_reg.oob_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
